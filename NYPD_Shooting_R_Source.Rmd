---
title: "DS_Week3_NYPD"
output: pdf_document
date: "2023-08-08"
---

# Start an Rmd Document

We can start this project by loading in a few libraries and then by loading in our data. The dataset comes from the cityofnewyork.us website as provided by the course and contains a record of information on shootings in NYC. The goal of this project will be to look into which people are most likely to be victims in NYC shootings.

```{r echo=T, results='hide', message=FALSE}
library(ggplot2)
library(lessR)
library(lubridate)
library("tidyverse")
```

```{r echo=T, results='hide', message=FALSE}
file_url <- "https://data.cityofnewyork.us/api/views/833y-fsy8/"
file_names <- c("rows.csv")
urls <- str_c(file_url, file_names)
nypd_data <- read_csv(urls)
```

```{r}
summary(nypd_data)
```

# Tidy and Transform Data

I can remove a good of columns that contain information I will not need. Mainly fields such as Longitude and Latitude, I can also remove columns with specific NYPD codes such as Jurisdiction Codes and Precinct. Having information about which Borough the crime happened in is enough for this project. We can also see from the summary above that the Occur Date is a string, we can transform that into a proper Date object.

```{r}
nypd_data <- nypd_data %>% select(-c(Latitude, Longitude, Lon_Lat, X_COORD_CD, 
                                     Y_COORD_CD, INCIDENT_KEY))
nypd_data <- nypd_data %>% select(-c(PRECINCT, JURISDICTION_CODE, LOCATION_DESC))
nypd_data$OCCUR_DATE <- mdy(nypd_data$OCCUR_DATE)
```

```{r}
summary(nypd_data)
```

```{r}
colMeans_df <- stack((colMeans(is.na(nypd_data)))*100)
plot1 <- ggplot(colMeans_df, aes(y=ind,x=values)) + geom_col() + 
  labs(title="Missing Values", y="Column", x="% Missing Values")
plot1
```

We can also see that over 75% of values are missing from LOC_CLASSFCTN_DESC and LOC_OF_OCCUR_DESC, so we can remove those too. We can also see that information is missing for over 25% of perpetrators, however we will leave the missing values and will not be augmenting any data in this project.

```{r}
nypd_data <- nypd_data %>% select(-c(LOC_CLASSFCTN_DESC, LOC_OF_OCCUR_DESC))
```

# Visualization and Analysis

```{r}
ggplot(nypd_data, aes(x=BORO, fill=VIC_RACE)) + geom_bar() + 
  labs(title='Number of victims by race', x='Borough', y='Number of victims', fill='Race') 
```

In this graph we can see black victims are the most common. It would be nice if this dataset contained information about the population in NYC so we could compare the percentage of black citizens in these areas to the percent of black victims, as well as the other races.

```{r}
ggplot(nypd_data, aes(x=BORO, fill=VIC_AGE_GROUP)) + geom_bar() + 
  labs(title='Number of victims by age', x='Borough', y='Number of victims', fill='Age') 
```

This graph shows the number of victims by their age group. Victims aged 25 to 44 appear to be the most common.

```{r echo=T, results='hide', message=FALSE}
VicSex_tb <- table(nypd_data$VIC_SEX)
PieChart(VicSex_tb, hole=0, values="%", main="Victims sex by percentage")
```

This pie chart shows the percentage of victims by their sex. Assuming NYC has a 50/50 split between males and females; male victims are overwhelmingly more likely than female victims.

```{r}
grouped_tb <- nypd_data %>%
  group_by(VIC_RACE, VIC_AGE_GROUP) %>%
  summarise(total_count=n(),.groups = 'drop')

model <- lm(total_count ~ VIC_RACE + VIC_AGE_GROUP, data=grouped_tb)
summary(model)
```

Here we try to use the victims race and age group as predictors for how likely a person is to be a victim of a shooting. From this we can see that black victims between the age of 25 and 44 are the most likely to be victims. This matches up with the visualizations we saw earlier.

# Bias Identification

Addressing bias is very important in any data science project, especially a project with political implications such as this one. As someone who lives in the NYC area it can be very easy for me to feel like I should represent this data in a more positive light, leading to bias. However, I corrected this bias by treating it like I would any other data science project and by not focusing on where the data came from and just focusing on how to represent what the data is showing us. I believe I handled and presented this data in an unbiased way, letting the data speak for itself.






















